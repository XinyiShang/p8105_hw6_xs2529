---
title: "p8105_hw6_xs2529"
author: "Xinyi Shang"
date: "2023-11-30"
output: github_document
---

```{r setup, include=FALSE}

library(tidyverse)
library(boot)
library(broom)
library(ggplot2)
library(modelr)
library(viridis)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

### Problem 1

The data cleaning process involved creating new variables (`city_state` and `resolution`), transforming `victim_age` to numeric, filtering `victim_race` to white and black and filtering out specific cities.

```{r q1_data_cleaning}
homicide_df = 
  read_csv("data/homicide-data.csv", na = c("", "NA", "Unknown")) |> 
  mutate(
    city_state = str_c(city, state, sep = ", "),
    victim_age = as.numeric(victim_age),
    resolution = case_when(
      disposition == "Closed without arrest" ~ 0,
      disposition == "Open/No arrest"        ~ 0,
      disposition == "Closed by arrest"      ~ 1)
  ) |> 
  filter(victim_race %in% c("White", "Black")) |> 
  filter(!(city_state %in% c("Tulsa, AL", "Dallas, TX", "Phoenix, AZ", "Kansas City, MO"))) |> 
  select(city_state, resolution, victim_age, victim_sex, victim_race)
```

Fit a logistic regression model using only data from Baltimore, MD. 

```{r q1_glm_baltimore}
baltimore_glm = 
  filter(homicide_df, city_state == "Baltimore, MD") |> 
  glm(resolution ~ victim_age + victim_sex + victim_race, family = binomial(), data = _)

baltimore_glm |> 
  broom::tidy() |> 
  mutate(
    OR = exp(estimate), 
    OR_CI_upper = exp(estimate + 1.96 * std.error),
    OR_CI_lower = exp(estimate - 1.96 * std.error)) |> 
  filter(term == "victim_sexMale") |> 
  select(OR, OR_CI_lower, OR_CI_upper) |>
  knitr::kable(digits = 3)
```

Fit a model for each of the cities.

```{r q1_glm_all_cities}
model_results = 
  homicide_df |> 
  nest(data = -city_state) |> 
  mutate(
    models = map(data, \(df) glm(resolution ~ victim_age + victim_sex + victim_race, 
                             family = binomial(), data = df)),
    tidy_models = map(models, broom::tidy)) |> 
  select(-models, -data) |> 
  unnest(cols = tidy_models) |> 
  mutate(
    OR = exp(estimate), 
    OR_CI_upper = exp(estimate + 1.96 * std.error),
    OR_CI_lower = exp(estimate - 1.96 * std.error)) |> 
  filter(term == "victim_sexMale") |> 
  select(city_state, OR, OR_CI_lower, OR_CI_upper)

model_results |>
  slice(1:5) |> 
  knitr::kable(digits = 3)
```

Generate a plot of the estimated ORs and CIs for each city, ordered by magnitude of the OR from smallest to largest.  

```{r q1_plot}
model_results |> 
  mutate(city_state = fct_reorder(city_state, OR)) |> 
  ggplot(aes(x = city_state, y = OR)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = OR_CI_lower, ymax = OR_CI_upper)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

Most cities show odds ratios below 1, indicating that crimes with male victims have lower resolution odds compared to female victims after adjusting for age and race. New York exhibits the strongest disparity. In roughly half of these cities, confidence intervals are narrow and do not contain 1, suggest a significant difference in resolution rates by sex after adjusting for age and race.

### Problem 2

```{r load data}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
```

Generating bootstrap samples, fitting linear regression models (tmax ~ tmin + prcp), and storing the beta coefficients and r-squared values from each model.

```{r}
n_bootstrap = 5000
set.seed(1)

boot_sample = function(df) {
  sample_frac(df, replace = TRUE)
}

boot_straps = 
  tibble(strap_number = 1:n_bootstrap) |> 
  mutate(
    strap_sample = map(strap_number, \(i) boot_sample(df = weather_df))
  )

bootstrap_results = 
  boot_straps |> 
  mutate(
    models = map(strap_sample, \(weather_df) lm(tmax ~ tmin + prcp, data = weather_df) ),
    results_beta = map(models, broom::tidy),
    results_r_squared = map(models,broom::glance)
    ) |>
  select(-strap_sample, -models) 
```

Extracting $\hat{r}^2$ values and compute the 95% confidence interval. 

```{r r squared}
r_squared = bootstrap_results |>
  unnest(results_r_squared) |>
  janitor::clean_names() |>
  select(r_squared)
  
r_squared_CI = r_squared |>
  summarize(
    ci_lower = quantile(r_squared, 0.025), 
    ci_upper = quantile(r_squared, 0.975))

r_squared_CI
```

Extracting $\log(\hat{\beta}_1 \cdot \hat{\beta}_2)$ values and compute the 95% confidence interval. 

```{r log(beta1*beta2)}
beta = bootstrap_results |>
  unnest(results_beta) |>
  janitor::clean_names() |>
  select (-results_r_squared) |>
  select(strap_number, term,estimate)|>
  pivot_wider(names_from = term, values_from = estimate) |>
  mutate(log_beta = log(tmin*prcp))
  
log_beta_CI = beta |> 
  drop_na() |>
  summarize(
    ci_lower = quantile(log_beta, 0.025), 
    ci_upper = quantile(log_beta, 0.975))

log_beta_CI
```

Making density plot for $\hat{r}^2$ and $\log(\hat{\beta}_1 \cdot \hat{\beta}_2)$.

```{r}
ggplot(r_squared, aes(x = r_squared)) +
  geom_density() +
  ggtitle("Density Plot of R Squared") +
  xlab("R Squared") +
  ylab("Density")
```

The density plot of $\hat{r}^2$ shows a symmetrical, bell-shaped curve, with the majority of data clustered around the center. It follows the normal distribution. 

```{r}
ggplot(beta, aes(x = log_beta)) +
  geom_density() +
  ggtitle("Density Plot of log(Beta1*Beta2)") +
  xlab("log(Beta1*Beta2)") +
  ylab("Density")
```

The density plot of $\log(\hat{\beta}_1 \cdot \hat{\beta}_2)$ is negatively skewed, suggests that the majority of values are concentrated on the higher end, with a peak on the left, and there are some lower values contributing to the longer left tail. 

### Problem 3

```{r data cleaning for birthweight}
birthweight = read_csv("data/birthweight.csv")|>
  janitor::clean_names()

summary(birthweight)

NA_birthweight = sum(is.na(birthweight))
NA_birthweight

birthweight = birthweight |>
  mutate(babysex = as.factor(babysex)) |>
  mutate(malform = as.factor(malform)) |>
  mutate(frace = as.factor(frace)) |>
  mutate(mrace = as.factor(mrace))
```
In the process of data cleaning, I observed that there are no missing values (NA) in the `birthweight` dataset. The dataset comprises `r nrow(birthweight)` observations and `r ncol(birthweight)` variables, and the names  are: `r names(birthweight)`. As part of the cleaning process, I converted the variables `babysex`, `malform`, `frace`, and `mrace` into factors.

```{r}

model <- lm(bwt ~ mheight + ppwt + ppbmi + delwt + wtgain, data = birthweight)

summary(model)

```
I hypothesized that mother's size, including weight and height, are important in determining baby's weight. So, I included mom's height `mheight`, mom's pre-pregnancy weight `ppwt`, mom's pre pregnancy bmi `ppbmi`, mother’s weight at delivery (pounds) `delwt`, and mother’s weight gain during pregnancy (pounds) `wtgain` into the model.

```{r}
birthweight = birthweight |>
  add_predictions(model) |>
  add_residuals(model)
birthweight|>
  ggplot(aes(x = pred, y = resid)) +
  geom_point() +
  geom_smooth() +
  labs(title = "Residuals vs Predicted Values",
       x = "Predicted Values",
       y = "Residuals")

```

In this residual vs. predicted values plot, the data points exhibit a circular pattern centered around (3000, 0), indicating potential heteroscedasticity. This suggests that the variability of the residuals is not consistent across various levels of predicted values. Moreover, the circular pattern may imply a lack of linearity in the relationship between the predictor and the data.

```{r}
cv_df = crossv_mc(birthweight, 100) 

cv_df = 
  cv_df |> 
  mutate(
    model01  = map(train, \(df) lm(bwt ~  mheight + ppwt + ppbmi + delwt + wtgain, data = birthweight)),
    model02     = map(train, \(df) lm(bwt ~ blength + gaweeks, data = birthweight)),
    model03  = map(train, \(df) lm(bwt ~ bhead * blength * babysex, data = birthweight))) |> 
  mutate(
    rmse_01 = map2_dbl(model01, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_02    = map2_dbl(model02, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_03 = map2_dbl(model03, test, \(mod, df) rmse(model = mod, data = df)))

```

Then, I conducted cross-validation to compare my proposed model and two given models:

Model_01: `mheight`, `ppwt`, `ppbmi`, `delwt`, `wtgain` (my proposal).

Model_02: `blength`, `gaweeks`.

Model_03: `bhead`, `blength`, `babysex`, and all interactions.

```{r}
cv_df |> 
  select(starts_with("rmse")) |> 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") |> 
  mutate(model = fct_inorder(model)) |> 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```

The Root Mean Square Error (RMSE) figure indicates that Model_03, which includes predictors `bhead`, `blength`, `babysex`, and all interactions, outperformed all other models by exhibiting the lowest RMSE values. However, the model I proposed (Model_01), which includes predictors `mheight`, `ppwt`, `ppbmi`, `delwt`, and `wtgain`, displayed the poorest performance as shown by its higher RMSE.

